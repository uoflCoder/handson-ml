{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What Linear Regression training algorithm should you use if you have a training set with millions of features?\n",
    "\n",
    "The two best algorithms to use are stochastic gradient descent and mini batch gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Suppose the features in your training set have very different scales. What algorithms might suffer from this. What can you do about this\n",
    "\n",
    "Gradient descent algorithms generally will suffer from features in the training set having very different scales. So you should always scale your training data set so that you can converge on your optimal solution quicker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Can Gradient Descent get stuck in a local minimum when training a Logistic Regression model?\n",
    "\n",
    "No because it is a convex function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Do all Gradient Descent algorithms lead to the same model provided you let them run long enough?\n",
    "Generally Yes. There are very few exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Suppose you use Batch Gradient Descent and you plot the validation error at every epoch. If you notice that the validation error consistently goes up, what is likely going on? How can you fix this?\n",
    "This means that the learning algorithm is overfitting to the training set. Things that you can do to fix this is include a regularization formula such as ridge, lasso, or elastic net. The most important thing you can do is implement early stopping. Also, it could mean that the learning rate is too high and the learning algorithm is diverging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Is it a good idea to stop Mini-batch Gradient Descent immediately when the validation error goes up?\n",
    "\n",
    "No. It is better to save the model at regular intervals and when it has not improve for a long time it is safe to say that you can revert to the best saved model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Which Gradient Descent algorithm in this chapter will reach the vicinity of the optimal solution the fastest? Which will actually converge? How can you make the others converge as well?\n",
    "Stochastic Gradient Descent because it makes a decision once per every training instance. Only Batch Gradient Descent will actually converge. You can make the other two (stochastic and mini batch gd) by gradually decreasing the learning rate as the learning algorithm begins gets closer and closer to the optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Suppose your are using Polynomial Regression. You plot the learning curves and you notice that there is a large gap between the training error and the validation error. What is happening? What are three ways to solve this?\n",
    "You are overfitting to the training set most likely. In order to solve this you can try these three things: Get more training data, Use regularization, or reduce the polynomial degree. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Suppose you are using Ridge Regression and you notice that the training error and the validation error are almost equal and fairly high. Would you say that the model suffers from high bias or high variance? Should you increase the regularization parameter or decrease it?\n",
    "High bias. Decrease regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Why would you want to use\n",
    "\n",
    "<li>Ridge Regression?</li>\n",
    "Ridge Regression would be good if your Linear Regression model is suffering from a high variance.\n",
    "\n",
    "<li>Lasso Regression?</li>\n",
    "Lasso Regression would be better if you think that there are \"useless variables\" that should be weighted to zero to get a more accurate result by finding these \"useless\" variables.\n",
    "\n",
    "<li>Elastic Net</li>\n",
    "Elastic Net is a happy medium between Ridge Regression and Lasso Regression. Usually perferred over Lasso Regression because Lasso Regression can behave erratically if some variables are highly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime. Shoudl you implement two Logistic Regression classifiers or one Softmax Regression classifiers?\n",
    "Two Logistic Regression classifiers because these classes are not mutually exclusive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip 12 because you won't need to do that ever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
